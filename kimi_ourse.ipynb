{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30aebeb",
   "metadata": {},
   "source": [
    "# 第一部分｜基础与大模型原理（1–6）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41eaaf",
   "metadata": {},
   "source": [
    "## 1. 请你简要介绍 Transformer 的 Self-Attention 机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e48a2",
   "metadata": {},
   "source": [
    "### 考察点：是否真正理解 Attention，而不是背公式\n",
    "\n",
    "一句话回答：\n",
    "Self-Attention 通过计算序列内部任意位置之间的相关性，使模型在一次前向中建模长距离依赖。\n",
    "\n",
    "展开要点：\n",
    "\n",
    "对输入做线性映射得到 Q、K、V\n",
    "\n",
    "权重 = softmax(QKᵀ / √d_k)\n",
    "\n",
    "加权求和得到新的表示\n",
    "\n",
    "多头 Attention 让模型在不同子空间关注不同关系\n",
    "\n",
    "相比 RNN：并行、高效、长依赖稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer-math",
   "metadata": {},
   "source": [
    "# Transformer架构数学公式推导\n",
    "\n",
    "## 1. 缩放点积注意力（Scaled Dot-Product Attention）\n",
    "\n",
    "缩放点积注意力是Transformer的核心组件，其计算过程如下：\n",
    "\n",
    "给定查询矩阵 $Q \\in \\mathbb{R}^{n \\times d_k}$、键矩阵 $K \\in \\mathbb{R}^{m \\times d_k}$ 和值矩阵 $V \\in \\mathbb{R}^{m \\times d_v}$，其中：\n",
    "- $n$ 是查询序列长度\n",
    "- $m$ 是键值序列长度\n",
    "- $d_k$ 是查询和键的维度\n",
    "- $d_v$ 是值的维度\n",
    "\n",
    "### 1.1 注意力权重计算\n",
    "\n",
    "首先计算查询和键的点积，然后进行缩放：\n",
    "\n",
    "$$\\text{scores} = \\frac{QK^T}{\\sqrt{d_k}} \\in \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "其中，缩放因子 $\\sqrt{d_k}$ 用于防止点积过大导致梯度消失。\n",
    "\n",
    "### 1.2 注意力权重归一化\n",
    "\n",
    "对注意力分数进行softmax归一化：\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "其中，softmax函数定义为：\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{m} e^{x_j}}$$\n",
    "\n",
    "### 1.3 带掩码的注意力\n",
    "\n",
    "在训练过程中，可能需要添加掩码（如解码器中的自回归掩码）：\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n",
    "\n",
    "其中，$M$ 是掩码矩阵，对于被掩码的位置，$M_{ij} = -\\infty$，否则 $M_{ij} = 0$。\n",
    "\n",
    "## 2. 多头自注意力机制（Multi-Head Attention）\n",
    "\n",
    "多头注意力将输入分割到不同的子空间，并行计算多个注意力头，然后合并结果。\n",
    "\n",
    "### 2.1 查询、键、值的线性变换\n",
    "\n",
    "给定输入 $X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$，其中 $d_{\\text{model}}$ 是模型维度：\n",
    "\n",
    "对于第 $i$ 个注意力头，使用不同的权重矩阵进行线性变换：\n",
    "\n",
    "$$Q_i = XW_i^Q, \\quad K_i = XW_i^K, \\quad V_i = XW_i^V$$\n",
    "\n",
    "其中：\n",
    "- $W_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "- $W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "- $W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$\n",
    "\n",
    "### 2.2 并行计算注意力头\n",
    "\n",
    "对每个注意力头计算注意力：\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i$$\n",
    "\n",
    "### 2.3 多头注意力合并\n",
    "\n",
    "将所有注意力头的输出拼接并通过线性变换：\n",
    "\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "其中：\n",
    "- $h$ 是注意力头的数量\n",
    "- $W^O \\in \\mathbb{R}^{h \\cdot d_v \\times d_{\\text{model}}}$ 是输出权重矩阵\n",
    "- 通常设置 $d_k = d_v = d_{\\text{model}}/h$\n",
    "\n",
    "## 3. 位置编码（Positional Encoding）\n",
    "\n",
    "由于Transformer不包含递归或卷积结构，需要显式地添加位置信息。\n",
    "\n",
    "### 3.1 正弦和余弦位置编码\n",
    "\n",
    "对于位置 $pos$ 和维度 $i$，位置编码定义为：\n",
    "\n",
    "$$PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "其中：\n",
    "- $pos$ 是位置索引（从0开始）\n",
    "- $i$ 是维度索引（从0开始）\n",
    "- $d_{\\text{model}}$ 是模型维度\n",
    "\n",
    "### 3.2 位置编码的性质\n",
    "\n",
    "这种位置编码具有以下重要性质：\n",
    "1. 对于任意固定的偏移量 $k$，$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性函数\n",
    "2. 相对位置关系在不同位置间保持一致\n",
    "\n",
    "### 3.3 位置编码的添加\n",
    "\n",
    "将位置编码加到输入嵌入中：\n",
    "\n",
    "$$X' = X + PE$$\n",
    "\n",
    "其中 $X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$ 是输入嵌入，$PE \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$ 是位置编码。\n",
    "\n",
    "## 4. 编码器-解码器结构（Encoder-Decoder Architecture）\n",
    "\n",
    "Transformer采用编码器-解码器架构，用于序列到序列的任务。\n",
    "\n",
    "### 4.1 编码器结构\n",
    "\n",
    "编码器由 $N$ 个相同的层堆叠而成，每层包含两个子层：\n",
    "\n",
    "$$\\text{EncoderLayer}(X) = \\text{LayerNorm}(X + \\text{MultiHeadAttention}(X, X, X))$$\n",
    "\n",
    "$$\\text{Output} = \\text{LayerNorm}(\\text{EncoderLayer}(X) + \\text{FeedForward}(\\text{EncoderLayer}(X)))$$\n",
    "\n",
    "### 4.2 解码器结构\n",
    "\n",
    "解码器也由 $N$ 个相同的层堆叠而成，每层包含三个子层：\n",
    "\n",
    "1. 掩码多头自注意力：\n",
    "$$\\text{MaskedMultiHeadAttention}(X, X, X)$$\n",
    "\n",
    "2. 编码器-解码器注意力：\n",
    "$$\\text{MultiHeadAttention}(X, \\text{Memory}, \\text{Memory})$$\n",
    "\n",
    "3. 前馈网络：\n",
    "$$\\text{FeedForward}(X)$$\n",
    "\n",
    "完整解码器层计算：\n",
    "\n",
    "$$\\text{DecoderLayer}(X) = \\text{LayerNorm}(X + \\text{MaskedMultiHeadAttention}(X, X, X))$$\n",
    "\n",
    "$$\\text{DecoderLayer}_2 = \\text{LayerNorm}(\\text{DecoderLayer}(X) + \\text{MultiHeadAttention}(\\text{DecoderLayer}(X), \\text{Memory}, \\text{Memory}))$$\n",
    "\n",
    "$$\\text{Output} = \\text{LayerNorm}(\\text{DecoderLayer}_2 + \\text{FeedForward}(\\text{DecoderLayer}_2))$$\n",
    "\n",
    "### 4.3 最终输出层\n",
    "\n",
    "解码器输出通过线性变换和softmax得到概率分布：\n",
    "\n",
    "$$\\text{Output} = \\text{Softmax}(\\text{Linear}(X))$$\n",
    "\n",
    "## 5. 前馈神经网络（Feed Forward Network）\n",
    "\n",
    "每个编码器和解码器层都包含一个前馈神经网络，该网络独立应用于每个位置。\n",
    "\n",
    "### 5.1 前馈网络结构\n",
    "\n",
    "前馈网络由两个线性变换和一个非线性激活函数组成：\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "其中：\n",
    "- $x \\in \\mathbb{R}^{d_{\\text{model}}}$ 是输入\n",
    "- $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{ff}}$ 是第一个线性变换的权重矩阵\n",
    "- $W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{\\text{model}}}$ 是第二个线性变换的权重矩阵\n",
    "- $b_1 \\in \\mathbb{R}^{d_{ff}}$ 和 $b_2 \\in \\mathbb{R}^{d_{\\text{model}}}$ 是偏置向量\n",
    "- $d_{ff}$ 是前馈网络的隐藏层维度，通常设置为 $4d_{\\text{model}}$\n",
    "- $\\text{max}(0, \\cdot)$ 是ReLU激活函数\n",
    "\n",
    "### 5.2 前馈网络的作用\n",
    "\n",
    "前馈网络的作用：\n",
    "1. 提供非线性变换能力\n",
    "2. 增强模型表达能力\n",
    "3. 在不同位置间独立处理信息\n",
    "\n",
    "## 6. 层归一化（Layer Normalization）\n",
    "\n",
    "层归一化是Transformer中的重要组件，用于稳定训练过程。\n",
    "\n",
    "### 6.1 层归一化定义\n",
    "\n",
    "对于输入 $x \\in \\mathbb{R}^{d_{\\text{model}}}$，层归一化定义为：\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "其中：\n",
    "- $\\mu = \\frac{1}{d_{\\text{model}}}\\sum_{i=1}^{d_{\\text{model}}} x_i$ 是均值\n",
    "- $\\sigma = \\sqrt{\\frac{1}{d_{\\text{model}}}\\sum_{i=1}^{d_{\\text{model}}}(x_i - \\mu)^2 + \\epsilon}$ 是标准差\n",
    "- $\\gamma \\in \\mathbb{R}^{d_{\\text{model}}}$ 是缩放参数\n",
    "- $\\beta \\in \\mathbb{R}^{d_{\\text{model}}}$ 是偏移参数\n",
    "- $\\epsilon$ 是一个小的常数，用于数值稳定性\n",
    "- $\\odot$ 表示逐元素乘法\n",
    "\n",
    "### 6.2 残差连接与层归一化\n",
    "\n",
    "Transformer中采用残差连接和层归一化：\n",
    "\n",
    "$$\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "\n",
    "其中，$\\text{Sublayer}(x)$ 是子层（如多头注意力或前馈网络）的输出。\n",
    "\n",
    "### 6.3 层归一化的优势\n",
    "\n",
    "层归一化的优势：\n",
    "1. 稳定训练过程\n",
    "2. 允许更高的学习率\n",
    "3. 减少对初始化的敏感性\n",
    "4. 加速收敛\n",
    "\n",
    "## 7. 完整Transformer模型\n",
    "\n",
    "### 7.1 编码器-解码器整体流程\n",
    "\n",
    "给定输入序列 $X = (x_1, x_2, \\ldots, x_n)$ 和目标序列 $Y = (y_1, y_2, \\ldots, y_m)$：\n",
    "\n",
    "1. 输入嵌入和位置编码：\n",
    "$$X_{\\text{embed}} = \\text{Embedding}(X) + \\text{PositionalEncoding}(n)$$\n",
    "$$Y_{\\text{embed}} = \\text{Embedding}(Y) + \\text{PositionalEncoding}(m)$$\n",
    "\n",
    "2. 编码器处理：\n",
    "$$H = \\text{Encoder}(X_{\\text{embed}})$$\n",
    "\n",
    "3. 解码器处理：\n",
    "$$Z = \\text{Decoder}(Y_{\\text{embed}}, H)$$\n",
    "\n",
    "4. 输出概率：\n",
    "$$P(Y|X) = \\text{Softmax}(\\text{Linear}(Z))$$\n",
    "\n",
    "### 7.2 训练目标\n",
    "\n",
    "Transformer使用交叉熵损失函数：\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{t=1}^{m} \\log P(y_t | y_{<t}, X)$$\n",
    "\n",
    "其中，$y_{<t} = (y_1, y_2, \\ldots, y_{t-1})$ 是目标序列的前缀。\n",
    "\n",
    "## 8. 复杂度分析\n",
    "\n",
    "### 8.1 计算复杂度\n",
    "\n",
    "对于序列长度为 $n$，模型维度为 $d$ 的Transformer：\n",
    "\n",
    "- 自注意力机制的计算复杂度：$\\mathcal{O}(n^2 \\cdot d)$\n",
    "- 前馈网络的计算复杂度：$\\mathcal{O}(n \\cdot d^2)$\n",
    "- 总体计算复杂度：$\\mathcal{O}(n^2 \\cdot d + n \\cdot d^2)$\n",
    "\n",
    "### 8.2 内存复杂度\n",
    "\n",
    "- 自注意力机制的内存复杂度：$\\mathcal{O}(n^2)$\n",
    "- 前馈网络的内存复杂度：$\\mathcal{O}(n \\cdot d)$\n",
    "\n",
    "## 9. 关键超参数\n",
    "\n",
    "Transformer的关键超参数及其典型值：\n",
    "\n",
    "| 超参数 | 符号 | 典型值 | 说明 |\n",
    "|--------|------|--------|------|\n",
    "| 模型维度 | $d_{\\text{model}}$ | 512 | 输入嵌入和输出的维度 |\n",
    "| 注意力头数 | $h$ | 8 | 多头注意力的头数 |\n",
    "| 前馈网络维度 | $d_{ff}$ | 2048 | 前馈网络隐藏层维度 |\n",
    "| 编码器层数 | $N$ | 6 | 编码器堆叠层数 |\n",
    "| 解码器层数 | $N$ | 6 | 解码器堆叠层数 |\n",
    "| Dropout率 | $p_{\\text{dropout}}$ | 0.1 | Dropout概率 |\n",
    "\n",
    "## 10. Transformer的变体与改进\n",
    "\n",
    "### 10.1 常见变体\n",
    "\n",
    "1. **Transformer-XL**：引入分段循环机制和相对位置编码\n",
    "2. **Universal Transformer**：引入自适应计算时间和深度\n",
    "3. **Longformer**：引入稀疏注意力机制\n",
    "4. **Reformer**：使用可逆层和局部敏感哈希注意力\n",
    "\n",
    "### 10.2 效率优化\n",
    "\n",
    "1. **稀疏注意力**：减少计算复杂度从 $\\mathcal{O}(n^2)$ 到 $\\mathcal{O}(n \\log n)$ 或更低\n",
    "2. **线性注意力**：使用核函数近似注意力机制\n",
    "3. **低秩近似**：使用矩阵分解减少参数量\n",
    "\n",
    "## 结论\n",
    "\n",
    "Transformer通过自注意力机制、位置编码和前馈网络的组合，实现了强大的序列建模能力。其数学基础清晰，结构设计优雅，已成为自然语言处理和其他序列任务的基础架构。通过理解其数学公式和计算过程，可以更好地应用和改进这一架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ed4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入: torch.Size([2, 5, 512])\n",
      "输出: torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    缩放点积多头注意力（无 Mask，无 Dropout，无 bias，方便手撕）\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads   # 每头的维度\n",
    "\n",
    "        # 统一用一个大矩阵一次性映射 Q/K/V，再拆多头\n",
    "        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.W_o   = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len, d_model]\n",
    "        return: 同形状输出\n",
    "        \"\"\"\n",
    "        batch, seq_len, d_model = x.size()\n",
    "\n",
    "        # 1. 线性映射 + 拆成 (batch, n_heads, seq_len, d_k)\n",
    "        qkv = self.W_qkv(x)                      # [B, L, 3D]\n",
    "        qkv = qkv.view(batch, seq_len, self.n_heads, 3 * self.d_k)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)            # [B, H, L, 3D/H]\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)           # 各 [B, H, L, d_k]\n",
    "\n",
    "        # 2. 缩放点积注意力\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # [B, H, L, L]\n",
    "        attn_weights = torch.softmax(scores, dim=-1)                         # 归一化\n",
    "        attn_out = torch.matmul(attn_weights, V)                             # [B, H, L, d_k]\n",
    "\n",
    "        # 3. 多头拼接 + 输出映射\n",
    "        attn_out = attn_out.permute(0, 2, 1, 3).contiguous()                 # [B, L, H, d_k]\n",
    "        attn_out = attn_out.view(batch, seq_len, d_model)                    # [B, L, D]\n",
    "        return self.W_o(attn_out)\n",
    "\n",
    "\n",
    "# ===== 测试 =====\n",
    "if __name__ == \"__main__\":\n",
    "    B, L, D = 2, 5, 512\n",
    "    x = torch.randn(B, L, D)\n",
    "    mha = MultiHeadAttention(d_model=D, n_heads=8)\n",
    "    out = mha(x)\n",
    "    print(\"输入:\", x.shape)\n",
    "    print(\"输出:\", out.shape)   # 应与输入一致\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
